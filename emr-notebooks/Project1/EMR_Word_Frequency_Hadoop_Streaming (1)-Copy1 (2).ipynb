{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e004fea-3b0b-4589-b0b0-3783020024c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# EMR Hadoop Streaming Word Frequency (MapReduce) — All-in-One Notebook - SYNDRONIZATION ISSUE FIXED\n",
    "\n",
    "This notebook runs an end-to-end **word frequency** job on **Amazon EMR** using **Hadoop Streaming**.\n",
    "\n",
    "It will:\n",
    "1. Create a small sample `logs.txt`\n",
    "2. Write `mapper.py` and `reducer.py`\n",
    "3. Upload input + code to S3\n",
    "4. Run Hadoop Streaming (`hadoop-streaming.jar`)\n",
    "5. Download and preview results\n",
    "\n",
    "## Assumptions\n",
    "- You are running this notebook **on an EMR cluster** (e.g., EMR Studio / JupyterLab attached to a cluster)\n",
    "- `aws` CLI is available and your EMR role has S3 read/write permissions\n",
    "- `hadoop` CLI is available\n",
    "- Hadoop streaming jar exists at `/usr/lib/hadoop-mapreduce/hadoop-streaming.jar`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff294be",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 0) Configure your S3 paths\n",
    "Set your bucket and prefix. Output path must not already exist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7dfd6f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T01:05:47.575114Z",
     "iopub.status.busy": "2026-02-02T01:05:47.574941Z",
     "iopub.status.idle": "2026-02-02T01:05:47.611293Z",
     "shell.execute_reply": "2026-02-02T01:05:47.610925Z",
     "shell.execute_reply.started": "2026-02-02T01:05:47.575093Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad11bf1a10b0421d83d3d380a8b4eda7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3_INPUT : s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/input/\n",
      "S3_CODE  : s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/code/\n",
      "S3_OUTPUT: s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/output/"
     ]
    }
   ],
   "source": [
    "import os, subprocess, textwrap\n",
    "from pathlib import Path\n",
    "\n",
    "S3_BUCKET = os.environ.get('S3_BUCKET', 'aws-logs-346690756907-us-east-1')\n",
    "PREFIX = os.environ.get('MR_PREFIX', 'mapreduce/wordcount_demo')\n",
    "\n",
    "S3_BASE = f\"s3://{S3_BUCKET}/{PREFIX}\".rstrip('/')\n",
    "S3_INPUT = f\"{S3_BASE}/input/\"\n",
    "S3_CODE = f\"{S3_BASE}/code/\"\n",
    "S3_OUTPUT = f\"{S3_BASE}/output/\"\n",
    "\n",
    "print('S3_INPUT :', S3_INPUT)\n",
    "print('S3_CODE  :', S3_CODE)\n",
    "print('S3_OUTPUT:', S3_OUTPUT)\n",
    "\n",
    "if 'YOUR_BUCKET_NAME' in S3_BUCKET:\n",
    "    print('\\n⚠️  Set S3_BUCKET before running upload/job steps.')\n",
    "\n",
    "def run(cmd, check=True):\n",
    "    print('»', ' '.join(cmd))\n",
    "    p = subprocess.run(cmd, text=True, capture_output=True)\n",
    "    if p.stdout:\n",
    "        print(p.stdout)\n",
    "    if p.stderr:\n",
    "        print(p.stderr)\n",
    "    if check and p.returncode != 0:\n",
    "        raise RuntimeError(f\"Command failed with exit code {p.returncode}\")\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ced403",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1) Sanity checks\n",
    "Verify we can find `aws`, `hadoop`, and the Hadoop Streaming jar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96c57458",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T01:05:50.487404Z",
     "iopub.status.busy": "2026-02-02T01:05:50.487237Z",
     "iopub.status.idle": "2026-02-02T01:05:50.520993Z",
     "shell.execute_reply": "2026-02-02T01:05:50.520623Z",
     "shell.execute_reply.started": "2026-02-02T01:05:50.487384Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34fc1a73f49249cfb69b8d0138409915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? which aws\n",
      "/usr/bin/aws\n",
      "\n",
      "? which hadoop\n",
      "/usr/bin/hadoop\n",
      "\n",
      "Streaming jar exists: True - /usr/lib/hadoop-mapreduce/hadoop-streaming.jar"
     ]
    }
   ],
   "source": [
    "run(['which','aws'], check=False)\n",
    "run(['which','hadoop'], check=False)\n",
    "\n",
    "STREAMING_JAR = Path('/usr/lib/hadoop-mapreduce/hadoop-streaming.jar')\n",
    "print('Streaming jar exists:', STREAMING_JAR.exists(), '-', str(STREAMING_JAR))\n",
    "if not STREAMING_JAR.exists():\n",
    "    print('\\n⚠️ Streaming jar path not found. Try:')\n",
    "    print('   sudo find /usr/lib -name \"*streaming*.jar\" | head')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcf74c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2) Create sample input data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34f7a21b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T01:05:55.561594Z",
     "iopub.status.busy": "2026-02-02T01:05:55.561428Z",
     "iopub.status.idle": "2026-02-02T01:05:55.595473Z",
     "shell.execute_reply": "2026-02-02T01:05:55.595081Z",
     "shell.execute_reply.started": "2026-02-02T01:05:55.561574Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56767cc569784ec18d29375abaaf93f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world hello\n",
      "MapReduce makes scaling easier\n",
      "Hello EMR world\n",
      "Race conditions happen without synchronization"
     ]
    }
   ],
   "source": [
    "logs = textwrap.dedent('''\\\n",
    "Hello world hello\n",
    "MapReduce makes scaling easier\n",
    "Hello EMR world\n",
    "Race conditions happen without synchronization\n",
    "''')\n",
    "with open('logs.txt','w',encoding='utf-8') as f:\n",
    "    f.write(logs)\n",
    "print(logs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e4c60d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3) Write the mapper and reducer\n",
    "- Mapper emits `(word, 1)`\n",
    "- Reducer sums counts per word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b81b617",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T01:06:03.017362Z",
     "iopub.status.busy": "2026-02-02T01:06:03.017190Z",
     "iopub.status.idle": "2026-02-02T01:06:03.051883Z",
     "shell.execute_reply": "2026-02-02T01:06:03.051525Z",
     "shell.execute_reply.started": "2026-02-02T01:06:03.017340Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5149725daf4b4155b1e605ce89ededd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? chmod +x mapper.py reducer.py\n",
      "Wrote mapper.py and reducer.py"
     ]
    }
   ],
   "source": [
    "mapper_py = textwrap.dedent('''\\\n",
    "    #!/usr/bin/env python3\n",
    "    import sys\n",
    "    import re\n",
    "\n",
    "    WORD_RE = re.compile(r\"[A-Za-z0-9']+\")\n",
    "\n",
    "    for line in sys.stdin:\n",
    "        for word in WORD_RE.findall(line.lower()):\n",
    "            print(f\"{word}\\t1\")\n",
    "''').lstrip()\n",
    "\n",
    "reducer_py = textwrap.dedent('''\\\n",
    "    #!/usr/bin/env python3\n",
    "    import sys\n",
    "\n",
    "    current_word = None\n",
    "    current_count = 0\n",
    "\n",
    "    for line in sys.stdin:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        word, count = line.split(\"\\t\", 1)\n",
    "        count = int(count)\n",
    "\n",
    "        if current_word == word:\n",
    "            current_count += count\n",
    "        else:\n",
    "            if current_word is not None:\n",
    "                print(f\"{current_word}\\t{current_count}\")\n",
    "            current_word = word\n",
    "            current_count = count\n",
    "\n",
    "    if current_word is not None:\n",
    "        print(f\"{current_word}\\t{current_count}\")\n",
    "''').lstrip()\n",
    "\n",
    "with open('mapper.py','w',encoding='utf-8') as f:\n",
    "    f.write(mapper_py)\n",
    "with open('reducer.py','w',encoding='utf-8') as f:\n",
    "    f.write(reducer_py)\n",
    "\n",
    "run(['chmod','+x','mapper.py','reducer.py'], check=False)\n",
    "print('Wrote mapper.py and reducer.py')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b410ee93",
   "metadata": {},
   "source": [
    "## 4) Quick local test (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce2a1770",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T01:06:06.469942Z",
     "iopub.status.busy": "2026-02-02T01:06:06.469773Z",
     "iopub.status.idle": "2026-02-02T01:06:06.710325Z",
     "shell.execute_reply": "2026-02-02T01:06:06.709884Z",
     "shell.execute_reply.started": "2026-02-02T01:06:06.469923Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ccadba553a4bd6a5793b1c8a3ccd99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? cat logs.txt | ./mapper.py | sort | ./reducer.py | sort -k2,2nr | head\n",
      "hello\t3\n",
      "world\t2\n",
      "conditions\t1\n",
      "easier\t1\n",
      "emr\t1\n",
      "happen\t1\n",
      "makes\t1\n",
      "mapreduce\t1\n",
      "race\t1\n",
      "scaling\t1"
     ]
    }
   ],
   "source": [
    "cmd = \"cat logs.txt | ./mapper.py | sort | ./reducer.py | sort -k2,2nr | head\"\n",
    "print('»', cmd)\n",
    "p = subprocess.run(cmd, shell=True, text=True, capture_output=True)\n",
    "print(p.stdout)\n",
    "if p.stderr:\n",
    "    print(p.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc5a862",
   "metadata": {},
   "source": [
    "## 5) Upload input + code to S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f1531e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T01:06:10.098670Z",
     "iopub.status.busy": "2026-02-02T01:06:10.098504Z",
     "iopub.status.idle": "2026-02-02T01:06:12.358247Z",
     "shell.execute_reply": "2026-02-02T01:06:12.357866Z",
     "shell.execute_reply.started": "2026-02-02T01:06:10.098652Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37460923bc254568a9973a512bd6ac30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? aws s3 cp logs.txt s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/input/\n",
      "Completed 112 Bytes/112 Bytes (2.2 KiB/s) with 1 file(s) remaining\n",
      "upload: ./logs.txt to s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/input/logs.txt\n",
      "\n",
      "? aws s3 cp mapper.py s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/code/\n",
      "Completed 182 Bytes/182 Bytes (3.2 KiB/s) with 1 file(s) remaining\n",
      "upload: ./mapper.py to s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/code/mapper.py\n",
      "\n",
      "? aws s3 cp reducer.py s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/code/\n",
      "Completed 509 Bytes/509 Bytes (13.4 KiB/s) with 1 file(s) remaining\n",
      "upload: ./reducer.py to s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/code/reducer.py\n",
      "\n",
      "Uploaded input and code to S3."
     ]
    }
   ],
   "source": [
    "if 'YOUR_BUCKET_NAME' in S3_BUCKET:\n",
    "    raise ValueError('Set S3_BUCKET to a real bucket name first (or export S3_BUCKET).')\n",
    "\n",
    "run(['aws','s3','cp','logs.txt', S3_INPUT])\n",
    "run(['aws','s3','cp','mapper.py', S3_CODE])\n",
    "run(['aws','s3','cp','reducer.py', S3_CODE])\n",
    "\n",
    "\n",
    "print('Uploaded input and code to S3.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55c785b",
   "metadata": {},
   "source": [
    "## 6) Run the Hadoop Streaming job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06c01bdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T01:06:14.766886Z",
     "iopub.status.busy": "2026-02-02T01:06:14.766722Z",
     "iopub.status.idle": "2026-02-02T01:06:40.105672Z",
     "shell.execute_reply": "2026-02-02T01:06:40.105309Z",
     "shell.execute_reply.started": "2026-02-02T01:06:14.766868Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "049e4bd08cc947c78cd25491710103f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? aws s3 rm s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/output/ --recursive\n",
      "delete: s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/output/_SUCCESS\n",
      "delete: s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/output/part-00000\n",
      "delete: s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/output/part-00002\n",
      "delete: s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/output/part-00001\n",
      "\n",
      "? hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -D mapreduce.job.name=wordcount-streaming -files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py -input s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/input/ -output s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/output/\n",
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-3.4.1-amzn-4.jar] /tmp/streamjob8998808290342822771.jar tmpDir=null\n",
      "\n",
      "2026-02-02 01:06:16,195 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-35-47.ec2.internal/172.31.35.47:8032\n",
      "2026-02-02 01:06:16,253 INFO client.AHSProxy: Connecting to Application History server at ip-172-31-35-47.ec2.internal/172.31.35.47:10200\n",
      "2026-02-02 01:06:16,279 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-35-47.ec2.internal/172.31.35.47:8032\n",
      "2026-02-02 01:06:16,280 INFO client.AHSProxy: Connecting to Application History server at ip-172-31-35-47.ec2.internal/172.31.35.47:10200\n",
      "2026-02-02 01:06:16,346 INFO s3a.EMRFSToS3AConfigMapping: Mapping EMRFS config fs.s3.sts.endpoint to S3A config fs.s3a.assumed.role.sts.endpoint\n",
      "2026-02-02 01:06:16,347 INFO s3a.EMRFSToS3AConfigMapping: Mapping EMRFS config fs.s3.buffer.dir to S3A config fs.s3a.buffer.dir\n",
      "2026-02-02 01:06:16,397 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2026-02-02 01:06:16,443 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 300 second(s).\n",
      "2026-02-02 01:06:16,443 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "2026-02-02 01:06:17,487 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/yarn/.staging/job_1769992841731_0002\n",
      "2026-02-02 01:06:17,684 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library\n",
      "2026-02-02 01:06:17,687 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 049362b7cf53ff5f739d6b1532457f2c6cd495e8]\n",
      "2026-02-02 01:06:17,798 INFO mapred.FileInputFormat: Total input files to process : 2\n",
      "2026-02-02 01:06:17,836 INFO mapreduce.JobSubmitter: number of splits:9\n",
      "2026-02-02 01:06:17,950 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1769992841731_0002\n",
      "2026-02-02 01:06:18,003 INFO mapreduce.JobSubmitter: Executing with tokens: [Kind: YARN_AM_RM_TOKEN, Service: , Ident: (appAttemptId { application_id { id: 1 cluster_timestamp: 1769992841731 } attemptId: 1 } keyId: -564142661)]\n",
      "2026-02-02 01:06:18,071 INFO conf.Configuration: resource-types.xml not found\n",
      "2026-02-02 01:06:18,072 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2026-02-02 01:06:18,104 INFO impl.YarnClientImpl: Submitted application application_1769992841731_0002\n",
      "2026-02-02 01:06:18,125 INFO mapreduce.Job: The url to track the job: http://ip-172-31-35-47.ec2.internal:20888/proxy/application_1769992841731_0002/\n",
      "2026-02-02 01:06:18,125 INFO mapreduce.Job: Running job: job_1769992841731_0002\n",
      "2026-02-02 01:06:22,174 INFO mapreduce.Job: Job job_1769992841731_0002 running in uber mode : false\n",
      "2026-02-02 01:06:22,174 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2026-02-02 01:06:27,239 INFO mapreduce.Job:  map 22% reduce 0%\n",
      "2026-02-02 01:06:28,254 INFO mapreduce.Job:  map 44% reduce 0%\n",
      "2026-02-02 01:06:29,260 INFO mapreduce.Job:  map 56% reduce 0%\n",
      "2026-02-02 01:06:31,271 INFO mapreduce.Job:  map 78% reduce 0%\n",
      "2026-02-02 01:06:32,275 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2026-02-02 01:06:34,284 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "2026-02-02 01:06:35,289 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "2026-02-02 01:06:37,297 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2026-02-02 01:06:39,312 INFO mapreduce.Job: Job job_1769992841731_0002 completed successfully\n",
      "2026-02-02 01:06:39,364 INFO mapreduce.Job: Counters: 59\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=39185\n",
      "\t\tFILE: Number of bytes written=4061684\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1160\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tS3: Number of bytes read=342584\n",
      "\t\tS3: Number of bytes written=28774\n",
      "\t\tS3: Number of read operations=9\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of write operations=6\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=9\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=9\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=191901120\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=82710144\n",
      "\t\tTotal time spent by all map tasks (ms)=32770\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7062\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=32770\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=7062\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=191901120\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=82710144\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3765\n",
      "\t\tMap output records=30684\n",
      "\t\tMap output bytes=215919\n",
      "\t\tMap output materialized bytes=71802\n",
      "\t\tInput split bytes=1160\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3048\n",
      "\t\tReduce shuffle bytes=71802\n",
      "\t\tReduce input records=30684\n",
      "\t\tReduce output records=3048\n",
      "\t\tSpilled Records=61368\n",
      "\t\tShuffled Maps =27\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=27\n",
      "\t\tGC time elapsed (ms)=363\n",
      "\t\tCPU time spent (ms)=12440\n",
      "\t\tPhysical memory (bytes) snapshot=6580125696\n",
      "\t\tVirtual memory (bytes) snapshot=99641917440\n",
      "\t\tTotal committed heap usage (bytes)=5683281920\n",
      "\t\tPeak Map Physical memory (bytes)=614289408\n",
      "\t\tPeak Map Virtual memory (bytes)=7039422464\n",
      "\t\tPeak Reduce Physical memory (bytes)=403738624\n",
      "\t\tPeak Reduce Virtual memory (bytes)=12130639872\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=342584\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=28774\n",
      "2026-02-02 01:06:39,364 INFO streaming.StreamJob: Output directory: s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/output/\n",
      "2026-02-02 01:06:39,368 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "2026-02-02 01:06:39,368 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "2026-02-02 01:06:39,368 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n",
      "\n",
      "CompletedProcess(args=['hadoop', 'jar', '/usr/lib/hadoop-mapreduce/hadoop-streaming.jar', '-D', 'mapreduce.job.name=wordcount-streaming', '-files', 'mapper.py,reducer.py', '-mapper', 'mapper.py', '-reducer', 'reducer.py', '-input', 's3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/input/', '-output', 's3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/output/'], returncode=0, stdout='packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-3.4.1-amzn-4.jar] /tmp/streamjob8998808290342822771.jar tmpDir=null\\n', stderr=\"2026-02-02 01:06:16,195 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-35-47.ec2.internal/172.31.35.47:8032\\n2026-02-02 01:06:16,253 INFO client.AHSProxy: Connecting to Application History server at ip-172-31-35-47.ec2.internal/172.31.35.47:10200\\n2026-02-02 01:06:16,279 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-35-47.ec2.internal/172.31.35.47:8032\\n2026-02-02 01:06:16,280 INFO client.AHSProxy: Connecting to Application History server at ip-172-31-35-47.ec2.internal/172.31.35.47:10200\\n2026-02-02 01:06:16,346 INFO s3a.EMRFSToS3AConfigMapping: Mapping EMRFS config fs.s3.sts.endpoint to S3A config fs.s3a.assumed.role.sts.endpoint\\n2026-02-02 01:06:16,347 INFO s3a.EMRFSToS3AConfigMapping: Mapping EMRFS config fs.s3.buffer.dir to S3A config fs.s3a.buffer.dir\\n2026-02-02 01:06:16,397 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\\n2026-02-02 01:06:16,443 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 300 second(s).\\n2026-02-02 01:06:16,443 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\\n2026-02-02 01:06:17,487 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/yarn/.staging/job_1769992841731_0002\\n2026-02-02 01:06:17,684 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library\\n2026-02-02 01:06:17,687 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 049362b7cf53ff5f739d6b1532457f2c6cd495e8]\\n2026-02-02 01:06:17,798 INFO mapred.FileInputFormat: Total input files to process : 2\\n2026-02-02 01:06:17,836 INFO mapreduce.JobSubmitter: number of splits:9\\n2026-02-02 01:06:17,950 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1769992841731_0002\\n2026-02-02 01:06:18,003 INFO mapreduce.JobSubmitter: Executing with tokens: [Kind: YARN_AM_RM_TOKEN, Service: , Ident: (appAttemptId { application_id { id: 1 cluster_timestamp: 1769992841731 } attemptId: 1 } keyId: -564142661)]\\n2026-02-02 01:06:18,071 INFO conf.Configuration: resource-types.xml not found\\n2026-02-02 01:06:18,072 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\\n2026-02-02 01:06:18,104 INFO impl.YarnClientImpl: Submitted application application_1769992841731_0002\\n2026-02-02 01:06:18,125 INFO mapreduce.Job: The url to track the job: http://ip-172-31-35-47.ec2.internal:20888/proxy/application_1769992841731_0002/\\n2026-02-02 01:06:18,125 INFO mapreduce.Job: Running job: job_1769992841731_0002\\n2026-02-02 01:06:22,174 INFO mapreduce.Job: Job job_1769992841731_0002 running in uber mode : false\\n2026-02-02 01:06:22,174 INFO mapreduce.Job:  map 0% reduce 0%\\n2026-02-02 01:06:27,239 INFO mapreduce.Job:  map 22% reduce 0%\\n2026-02-02 01:06:28,254 INFO mapreduce.Job:  map 44% reduce 0%\\n2026-02-02 01:06:29,260 INFO mapreduce.Job:  map 56% reduce 0%\\n2026-02-02 01:06:31,271 INFO mapreduce.Job:  map 78% reduce 0%\\n2026-02-02 01:06:32,275 INFO mapreduce.Job:  map 100% reduce 0%\\n2026-02-02 01:06:34,284 INFO mapreduce.Job:  map 100% reduce 33%\\n2026-02-02 01:06:35,289 INFO mapreduce.Job:  map 100% reduce 67%\\n2026-02-02 01:06:37,297 INFO mapreduce.Job:  map 100% reduce 100%\\n2026-02-02 01:06:39,312 INFO mapreduce.Job: Job job_1769992841731_0002 completed successfully\\n2026-02-02 01:06:39,364 INFO mapreduce.Job: Counters: 59\\n\\tFile System Counters\\n\\t\\tFILE: Number of bytes read=39185\\n\\t\\tFILE: Number of bytes written=4061684\\n\\t\\tFILE: Number of read operations=0\\n\\t\\tFILE: Number of large read operations=0\\n\\t\\tFILE: Number of write operations=0\\n\\t\\tHDFS: Number of bytes read=1160\\n\\t\\tHDFS: Number of bytes written=0\\n\\t\\tHDFS: Number of read operations=9\\n\\t\\tHDFS: Number of large read operations=0\\n\\t\\tHDFS: Number of write operations=0\\n\\t\\tHDFS: Number of bytes read erasure-coded=0\\n\\t\\tS3: Number of bytes read=342584\\n\\t\\tS3: Number of bytes written=28774\\n\\t\\tS3: Number of read operations=9\\n\\t\\tS3: Number of large read operations=0\\n\\t\\tS3: Number of write operations=6\\n\\tJob Counters \\n\\t\\tLaunched map tasks=9\\n\\t\\tLaunched reduce tasks=3\\n\\t\\tData-local map tasks=9\\n\\t\\tTotal time spent by all maps in occupied slots (ms)=191901120\\n\\t\\tTotal time spent by all reduces in occupied slots (ms)=82710144\\n\\t\\tTotal time spent by all map tasks (ms)=32770\\n\\t\\tTotal time spent by all reduce tasks (ms)=7062\\n\\t\\tTotal vcore-milliseconds taken by all map tasks=32770\\n\\t\\tTotal vcore-milliseconds taken by all reduce tasks=7062\\n\\t\\tTotal megabyte-milliseconds taken by all map tasks=191901120\\n\\t\\tTotal megabyte-milliseconds taken by all reduce tasks=82710144\\n\\tMap-Reduce Framework\\n\\t\\tMap input records=3765\\n\\t\\tMap output records=30684\\n\\t\\tMap output bytes=215919\\n\\t\\tMap output materialized bytes=71802\\n\\t\\tInput split bytes=1160\\n\\t\\tCombine input records=0\\n\\t\\tCombine output records=0\\n\\t\\tReduce input groups=3048\\n\\t\\tReduce shuffle bytes=71802\\n\\t\\tReduce input records=30684\\n\\t\\tReduce output records=3048\\n\\t\\tSpilled Records=61368\\n\\t\\tShuffled Maps =27\\n\\t\\tFailed Shuffles=0\\n\\t\\tMerged Map outputs=27\\n\\t\\tGC time elapsed (ms)=363\\n\\t\\tCPU time spent (ms)=12440\\n\\t\\tPhysical memory (bytes) snapshot=6580125696\\n\\t\\tVirtual memory (bytes) snapshot=99641917440\\n\\t\\tTotal committed heap usage (bytes)=5683281920\\n\\t\\tPeak Map Physical memory (bytes)=614289408\\n\\t\\tPeak Map Virtual memory (bytes)=7039422464\\n\\t\\tPeak Reduce Physical memory (bytes)=403738624\\n\\t\\tPeak Reduce Virtual memory (bytes)=12130639872\\n\\tShuffle Errors\\n\\t\\tBAD_ID=0\\n\\t\\tCONNECTION=0\\n\\t\\tIO_ERROR=0\\n\\t\\tWRONG_LENGTH=0\\n\\t\\tWRONG_MAP=0\\n\\t\\tWRONG_REDUCE=0\\n\\tFile Input Format Counters \\n\\t\\tBytes Read=342584\\n\\tFile Output Format Counters \\n\\t\\tBytes Written=28774\\n2026-02-02 01:06:39,364 INFO streaming.StreamJob: Output directory: s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/output/\\n2026-02-02 01:06:39,368 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\\n2026-02-02 01:06:39,368 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\\n2026-02-02 01:06:39,368 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\\n\")"
     ]
    }
   ],
   "source": [
    "# Optional cleanup so you can re-run without changing S3_OUTPUT\n",
    "run(['aws','s3','rm', S3_OUTPUT, '--recursive'], check=False)\n",
    "\n",
    "cmd = [\n",
    "    'hadoop','jar', str(STREAMING_JAR),\n",
    "    '-D','mapreduce.job.name=wordcount-streaming',\n",
    "    '-files','mapper.py,reducer.py',\n",
    "    '-mapper','mapper.py',\n",
    "    '-reducer','reducer.py',\n",
    "    '-input', S3_INPUT,\n",
    "    '-output', S3_OUTPUT,\n",
    "]\n",
    "run(cmd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a7afad",
   "metadata": {},
   "source": [
    "## 7) Read results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2415bc17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T01:06:47.396481Z",
     "iopub.status.busy": "2026-02-02T01:06:47.396311Z",
     "iopub.status.idle": "2026-02-02T01:06:49.653497Z",
     "shell.execute_reply": "2026-02-02T01:06:49.653118Z",
     "shell.execute_reply.started": "2026-02-02T01:06:47.396460Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad0e39fbdd74b14955b2637e53be493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Listing all reducer output files ===\n",
      "? aws s3 ls s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/output/\n",
      "2026-02-02 01:06:37          0 _SUCCESS\n",
      "2026-02-02 01:06:33       9840 part-00000\n",
      "2026-02-02 01:06:34       9780 part-00001\n",
      "2026-02-02 01:06:36       9154 part-00002\n",
      "\n",
      "\n",
      "============================================================\n",
      "DEMONSTRATING THE SYNCHRONIZATION ISSUE\n",
      "============================================================\n",
      "\n",
      "--- Reducer 0 Output (part-00000) ---\n",
      "First 10 words from this reducer:\n",
      "'as\t1\n",
      "11\t4\n",
      "1991\t1\n",
      "2\t3\n",
      "20\t1\n",
      "5\t4\n",
      "50\t1\n",
      "596\t1\n",
      "6221541\t1\n",
      "8\t5\n",
      "Total words in this reducer: 1037\n",
      "\n",
      "--- Reducer 1 Output (part-00001) ---\n",
      "First 10 words from this reducer:\n",
      "0\t3\n",
      "000\t1\n",
      "12\t1\n",
      "1500\t1\n",
      "1887\t1\n",
      "2001\t1\n",
      "3\t11\n",
      "30\t1\n",
      "501\t1\n",
      "6\t2\n",
      "Total words in this reducer: 1037\n",
      "\n",
      "--- Reducer 2 Output (part-00002) ---\n",
      "First 10 words from this reducer:\n",
      "1\t47\n",
      "2020\t1\n",
      "4\t5\n",
      "64\t1\n",
      "7\t3\n",
      "a\t695\n",
      "absence\t1\n",
      "accept\t1\n",
      "accessed\t1\n",
      "accessible\t1\n",
      "Total words in this reducer: 974\n",
      "\n",
      "============================================================\n",
      "COMBINED RESULTS (Manual Aggregation Required!)\n",
      "============================================================\n",
      "Total unique words: 3048\n",
      "\n",
      "Top 10 words (after aggregating all 3 reducers):\n",
      "the                  1839\n",
      "and                  942\n",
      "to                   811\n",
      "a                    695\n",
      "of                   638\n",
      "it                   610\n",
      "she                  553\n",
      "i                    546\n",
      "you                  486\n",
      "said                 462\n",
      "\n",
      "This is the SYNCHRONIZATION ISSUE: we had to manually combine\n",
      "results from all 3 reducers to get the full picture!"
     ]
    }
   ],
   "source": [
    "print(\"=== Listing all reducer output files ===\")\n",
    "run(['aws','s3','ls', S3_OUTPUT])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEMONSTRATING THE SYNCHRONIZATION ISSUE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Collect data from ALL reducers\n",
    "all_words = {}\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Reducer {i} Output (part-0000{i}) ---\")\n",
    "    \n",
    "    p = subprocess.run(['aws','s3','cp', f\"{S3_OUTPUT}part-0000{i}\", '-'], \n",
    "                       text=True, capture_output=True, check=False)\n",
    "    \n",
    "    if p.returncode == 0:\n",
    "        lines = p.stdout.strip().splitlines()\n",
    "        print(f\"First 10 words from this reducer:\")\n",
    "        for line in lines[:10]:\n",
    "            print(line)\n",
    "        print(f\"Total words in this reducer: {len(lines)}\")\n",
    "        \n",
    "        # Aggregate across all reducers\n",
    "        for line in lines:\n",
    "            if line:\n",
    "                word, count = line.split('\\t')\n",
    "                all_words[word] = all_words.get(word, 0) + int(count)\n",
    "\n",
    "# Now show the COMBINED results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMBINED RESULTS (Manual Aggregation Required!)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total unique words: {len(all_words)}\")\n",
    "\n",
    "# Top 10 words after combining all reducers\n",
    "sorted_words = sorted(all_words.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nTop 10 words (after aggregating all 3 reducers):\")\n",
    "for word, count in sorted_words[:10]:\n",
    "    print(f\"{word:20s} {count}\")\n",
    "\n",
    "print(\"\\nThis is the SYNCHRONIZATION ISSUE: we had to manually combine\")\n",
    "print(\"results from all 3 reducers to get the full picture!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5779fbb-e87b-48cd-acde-c92effff29cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8) Experiment with Combiner\n",
    "\n",
    "A combiner pre-aggregates data on mapper nodes before sending to reducers.\n",
    "This reduces network traffic and speeds up the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ca91663-7c27-4948-bcb0-a100842448cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T01:07:00.148899Z",
     "iopub.status.busy": "2026-02-02T01:07:00.148725Z",
     "iopub.status.idle": "2026-02-02T01:07:31.502995Z",
     "shell.execute_reply": "2026-02-02T01:07:31.502641Z",
     "shell.execute_reply.started": "2026-02-02T01:07:00.148874Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9819a8fa60b4fc5b14e537a2e6c4729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? aws s3 rm s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/output_combiner/ --recursive\n",
      "? hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -D mapreduce.job.name=wordcount-with-combiner -D mapreduce.job.reduces=3 -files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py -combiner reducer.py -input s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/input/ -output s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/output_combiner/\n",
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-3.4.1-amzn-4.jar] /tmp/streamjob18072558437374551796.jar tmpDir=null\n",
      "\n",
      "2026-02-02 01:07:01,477 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-35-47.ec2.internal/172.31.35.47:8032\n",
      "2026-02-02 01:07:01,533 INFO client.AHSProxy: Connecting to Application History server at ip-172-31-35-47.ec2.internal/172.31.35.47:10200\n",
      "2026-02-02 01:07:01,556 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-35-47.ec2.internal/172.31.35.47:8032\n",
      "2026-02-02 01:07:01,556 INFO client.AHSProxy: Connecting to Application History server at ip-172-31-35-47.ec2.internal/172.31.35.47:10200\n",
      "2026-02-02 01:07:01,609 INFO s3a.EMRFSToS3AConfigMapping: Mapping EMRFS config fs.s3.sts.endpoint to S3A config fs.s3a.assumed.role.sts.endpoint\n",
      "2026-02-02 01:07:01,610 INFO s3a.EMRFSToS3AConfigMapping: Mapping EMRFS config fs.s3.buffer.dir to S3A config fs.s3a.buffer.dir\n",
      "2026-02-02 01:07:01,660 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2026-02-02 01:07:01,706 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 300 second(s).\n",
      "2026-02-02 01:07:01,706 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "2026-02-02 01:07:02,734 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/yarn/.staging/job_1769992841731_0003\n",
      "2026-02-02 01:07:02,917 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library\n",
      "2026-02-02 01:07:02,919 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 049362b7cf53ff5f739d6b1532457f2c6cd495e8]\n",
      "2026-02-02 01:07:03,026 INFO mapred.FileInputFormat: Total input files to process : 2\n",
      "2026-02-02 01:07:03,468 INFO mapreduce.JobSubmitter: number of splits:9\n",
      "2026-02-02 01:07:03,551 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1769992841731_0003\n",
      "2026-02-02 01:07:03,601 INFO mapreduce.JobSubmitter: Executing with tokens: [Kind: YARN_AM_RM_TOKEN, Service: , Ident: (appAttemptId { application_id { id: 1 cluster_timestamp: 1769992841731 } attemptId: 1 } keyId: -564142661)]\n",
      "2026-02-02 01:07:03,661 INFO conf.Configuration: resource-types.xml not found\n",
      "2026-02-02 01:07:03,661 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2026-02-02 01:07:03,692 INFO impl.YarnClientImpl: Submitted application application_1769992841731_0003\n",
      "2026-02-02 01:07:03,710 INFO mapreduce.Job: The url to track the job: http://ip-172-31-35-47.ec2.internal:20888/proxy/application_1769992841731_0003/\n",
      "2026-02-02 01:07:03,711 INFO mapreduce.Job: Running job: job_1769992841731_0003\n",
      "2026-02-02 01:07:07,775 INFO mapreduce.Job: Job job_1769992841731_0003 running in uber mode : false\n",
      "2026-02-02 01:07:07,776 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2026-02-02 01:07:12,811 INFO mapreduce.Job:  map 11% reduce 0%\n",
      "2026-02-02 01:07:15,825 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2026-02-02 01:07:21,844 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2026-02-02 01:07:25,856 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "2026-02-02 01:07:28,866 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2026-02-02 01:07:29,874 INFO mapreduce.Job: Job job_1769992841731_0003 completed successfully\n",
      "2026-02-02 01:07:29,923 INFO mapreduce.Job: Counters: 59\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=37170\n",
      "\t\tFILE: Number of bytes written=4050049\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1160\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tS3: Number of bytes read=351552\n",
      "\t\tS3: Number of bytes written=28774\n",
      "\t\tS3: Number of read operations=9\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of write operations=6\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=9\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=9\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=251462496\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=97396992\n",
      "\t\tTotal time spent by all map tasks (ms)=42941\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8316\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=42941\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=8316\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=251462496\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=97396992\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3765\n",
      "\t\tMap output records=30684\n",
      "\t\tMap output bytes=215919\n",
      "\t\tMap output materialized bytes=57700\n",
      "\t\tInput split bytes=1160\n",
      "\t\tCombine input records=30684\n",
      "\t\tCombine output records=6786\n",
      "\t\tReduce input groups=3048\n",
      "\t\tReduce shuffle bytes=57700\n",
      "\t\tReduce input records=6786\n",
      "\t\tReduce output records=3048\n",
      "\t\tSpilled Records=13572\n",
      "\t\tShuffled Maps =27\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=27\n",
      "\t\tGC time elapsed (ms)=410\n",
      "\t\tCPU time spent (ms)=14110\n",
      "\t\tPhysical memory (bytes) snapshot=6603284480\n",
      "\t\tVirtual memory (bytes) snapshot=99640000512\n",
      "\t\tTotal committed heap usage (bytes)=5691670528\n",
      "\t\tPeak Map Physical memory (bytes)=631853056\n",
      "\t\tPeak Map Virtual memory (bytes)=7040028672\n",
      "\t\tPeak Reduce Physical memory (bytes)=408109056\n",
      "\t\tPeak Reduce Virtual memory (bytes)=12133523456\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=351552\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=28774\n",
      "2026-02-02 01:07:29,923 INFO streaming.StreamJob: Output directory: s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/output_combiner/\n",
      "2026-02-02 01:07:29,927 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "2026-02-02 01:07:29,927 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "2026-02-02 01:07:29,927 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n",
      "\n",
      "CompletedProcess(args=['hadoop', 'jar', '/usr/lib/hadoop-mapreduce/hadoop-streaming.jar', '-D', 'mapreduce.job.name=wordcount-with-combiner', '-D', 'mapreduce.job.reduces=3', '-files', 'mapper.py,reducer.py', '-mapper', 'mapper.py', '-reducer', 'reducer.py', '-combiner', 'reducer.py', '-input', 's3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/input/', '-output', 's3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/output_combiner/'], returncode=0, stdout='packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-3.4.1-amzn-4.jar] /tmp/streamjob18072558437374551796.jar tmpDir=null\\n', stderr=\"2026-02-02 01:07:01,477 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-35-47.ec2.internal/172.31.35.47:8032\\n2026-02-02 01:07:01,533 INFO client.AHSProxy: Connecting to Application History server at ip-172-31-35-47.ec2.internal/172.31.35.47:10200\\n2026-02-02 01:07:01,556 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-35-47.ec2.internal/172.31.35.47:8032\\n2026-02-02 01:07:01,556 INFO client.AHSProxy: Connecting to Application History server at ip-172-31-35-47.ec2.internal/172.31.35.47:10200\\n2026-02-02 01:07:01,609 INFO s3a.EMRFSToS3AConfigMapping: Mapping EMRFS config fs.s3.sts.endpoint to S3A config fs.s3a.assumed.role.sts.endpoint\\n2026-02-02 01:07:01,610 INFO s3a.EMRFSToS3AConfigMapping: Mapping EMRFS config fs.s3.buffer.dir to S3A config fs.s3a.buffer.dir\\n2026-02-02 01:07:01,660 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\\n2026-02-02 01:07:01,706 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 300 second(s).\\n2026-02-02 01:07:01,706 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\\n2026-02-02 01:07:02,734 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/yarn/.staging/job_1769992841731_0003\\n2026-02-02 01:07:02,917 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library\\n2026-02-02 01:07:02,919 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 049362b7cf53ff5f739d6b1532457f2c6cd495e8]\\n2026-02-02 01:07:03,026 INFO mapred.FileInputFormat: Total input files to process : 2\\n2026-02-02 01:07:03,468 INFO mapreduce.JobSubmitter: number of splits:9\\n2026-02-02 01:07:03,551 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1769992841731_0003\\n2026-02-02 01:07:03,601 INFO mapreduce.JobSubmitter: Executing with tokens: [Kind: YARN_AM_RM_TOKEN, Service: , Ident: (appAttemptId { application_id { id: 1 cluster_timestamp: 1769992841731 } attemptId: 1 } keyId: -564142661)]\\n2026-02-02 01:07:03,661 INFO conf.Configuration: resource-types.xml not found\\n2026-02-02 01:07:03,661 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\\n2026-02-02 01:07:03,692 INFO impl.YarnClientImpl: Submitted application application_1769992841731_0003\\n2026-02-02 01:07:03,710 INFO mapreduce.Job: The url to track the job: http://ip-172-31-35-47.ec2.internal:20888/proxy/application_1769992841731_0003/\\n2026-02-02 01:07:03,711 INFO mapreduce.Job: Running job: job_1769992841731_0003\\n2026-02-02 01:07:07,775 INFO mapreduce.Job: Job job_1769992841731_0003 running in uber mode : false\\n2026-02-02 01:07:07,776 INFO mapreduce.Job:  map 0% reduce 0%\\n2026-02-02 01:07:12,811 INFO mapreduce.Job:  map 11% reduce 0%\\n2026-02-02 01:07:15,825 INFO mapreduce.Job:  map 67% reduce 0%\\n2026-02-02 01:07:21,844 INFO mapreduce.Job:  map 100% reduce 0%\\n2026-02-02 01:07:25,856 INFO mapreduce.Job:  map 100% reduce 67%\\n2026-02-02 01:07:28,866 INFO mapreduce.Job:  map 100% reduce 100%\\n2026-02-02 01:07:29,874 INFO mapreduce.Job: Job job_1769992841731_0003 completed successfully\\n2026-02-02 01:07:29,923 INFO mapreduce.Job: Counters: 59\\n\\tFile System Counters\\n\\t\\tFILE: Number of bytes read=37170\\n\\t\\tFILE: Number of bytes written=4050049\\n\\t\\tFILE: Number of read operations=0\\n\\t\\tFILE: Number of large read operations=0\\n\\t\\tFILE: Number of write operations=0\\n\\t\\tHDFS: Number of bytes read=1160\\n\\t\\tHDFS: Number of bytes written=0\\n\\t\\tHDFS: Number of read operations=9\\n\\t\\tHDFS: Number of large read operations=0\\n\\t\\tHDFS: Number of write operations=0\\n\\t\\tHDFS: Number of bytes read erasure-coded=0\\n\\t\\tS3: Number of bytes read=351552\\n\\t\\tS3: Number of bytes written=28774\\n\\t\\tS3: Number of read operations=9\\n\\t\\tS3: Number of large read operations=0\\n\\t\\tS3: Number of write operations=6\\n\\tJob Counters \\n\\t\\tLaunched map tasks=9\\n\\t\\tLaunched reduce tasks=3\\n\\t\\tData-local map tasks=9\\n\\t\\tTotal time spent by all maps in occupied slots (ms)=251462496\\n\\t\\tTotal time spent by all reduces in occupied slots (ms)=97396992\\n\\t\\tTotal time spent by all map tasks (ms)=42941\\n\\t\\tTotal time spent by all reduce tasks (ms)=8316\\n\\t\\tTotal vcore-milliseconds taken by all map tasks=42941\\n\\t\\tTotal vcore-milliseconds taken by all reduce tasks=8316\\n\\t\\tTotal megabyte-milliseconds taken by all map tasks=251462496\\n\\t\\tTotal megabyte-milliseconds taken by all reduce tasks=97396992\\n\\tMap-Reduce Framework\\n\\t\\tMap input records=3765\\n\\t\\tMap output records=30684\\n\\t\\tMap output bytes=215919\\n\\t\\tMap output materialized bytes=57700\\n\\t\\tInput split bytes=1160\\n\\t\\tCombine input records=30684\\n\\t\\tCombine output records=6786\\n\\t\\tReduce input groups=3048\\n\\t\\tReduce shuffle bytes=57700\\n\\t\\tReduce input records=6786\\n\\t\\tReduce output records=3048\\n\\t\\tSpilled Records=13572\\n\\t\\tShuffled Maps =27\\n\\t\\tFailed Shuffles=0\\n\\t\\tMerged Map outputs=27\\n\\t\\tGC time elapsed (ms)=410\\n\\t\\tCPU time spent (ms)=14110\\n\\t\\tPhysical memory (bytes) snapshot=6603284480\\n\\t\\tVirtual memory (bytes) snapshot=99640000512\\n\\t\\tTotal committed heap usage (bytes)=5691670528\\n\\t\\tPeak Map Physical memory (bytes)=631853056\\n\\t\\tPeak Map Virtual memory (bytes)=7040028672\\n\\t\\tPeak Reduce Physical memory (bytes)=408109056\\n\\t\\tPeak Reduce Virtual memory (bytes)=12133523456\\n\\tShuffle Errors\\n\\t\\tBAD_ID=0\\n\\t\\tCONNECTION=0\\n\\t\\tIO_ERROR=0\\n\\t\\tWRONG_LENGTH=0\\n\\t\\tWRONG_MAP=0\\n\\t\\tWRONG_REDUCE=0\\n\\tFile Input Format Counters \\n\\t\\tBytes Read=351552\\n\\tFile Output Format Counters \\n\\t\\tBytes Written=28774\\n2026-02-02 01:07:29,923 INFO streaming.StreamJob: Output directory: s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/output_combiner/\\n2026-02-02 01:07:29,927 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\\n2026-02-02 01:07:29,927 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\\n2026-02-02 01:07:29,927 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\\n\")"
     ]
    }
   ],
   "source": [
    "# Add new S3 output path at the top of your notebook (in Section 0)\n",
    "S3_OUTPUT_COMBINER = f\"{S3_BASE}/output_combiner/\"\n",
    "\n",
    "# Run job with combiner\n",
    "run(['aws','s3','rm', S3_OUTPUT_COMBINER, '--recursive'], check=False)\n",
    "\n",
    "cmd = [\n",
    "    'hadoop','jar', str(STREAMING_JAR),\n",
    "    '-D','mapreduce.job.name=wordcount-with-combiner',\n",
    "    '-D','mapreduce.job.reduces=3',\n",
    "    '-files','mapper.py,reducer.py',\n",
    "    '-mapper','mapper.py',\n",
    "    '-reducer','reducer.py',\n",
    "    '-combiner','reducer.py',  # THIS IS THE NEW LINE!\n",
    "    '-input', S3_INPUT,\n",
    "    '-output', S3_OUTPUT_COMBINER,\n",
    "]\n",
    "run(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39eea5c2-8d05-4ad4-a4ca-9b25a4574217",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T01:07:35.568241Z",
     "iopub.status.busy": "2026-02-02T01:07:35.568075Z",
     "iopub.status.idle": "2026-02-02T01:07:37.822062Z",
     "shell.execute_reply": "2026-02-02T01:07:37.821677Z",
     "shell.execute_reply.started": "2026-02-02T01:07:35.568222Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4900943d0c164a11877bd329458e5337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Combiner Job Results ===\n",
      "? aws s3 ls s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/output_combiner/\n",
      "2026-02-02 01:07:28          0 _SUCCESS\n",
      "2026-02-02 01:07:25       9840 part-00000\n",
      "2026-02-02 01:07:25       9780 part-00001\n",
      "2026-02-02 01:07:28       9154 part-00002\n",
      "\n",
      "Total unique words with combiner: 3048\n",
      "\n",
      "Top 10 words with combiner:\n",
      "the                  1839\n",
      "and                  942\n",
      "to                   811\n",
      "a                    695\n",
      "of                   638\n",
      "it                   610\n",
      "she                  553\n",
      "i                    546\n",
      "you                  486\n",
      "said                 462\n",
      "\n",
      "Results match without combiner: True\n",
      "The combiner gives SAME results but with better performance!"
     ]
    }
   ],
   "source": [
    "print(\"=== Combiner Job Results ===\")\n",
    "run(['aws','s3','ls', S3_OUTPUT_COMBINER])\n",
    "\n",
    "# Aggregate combiner results\n",
    "combiner_words = {}\n",
    "for i in range(3):\n",
    "    p = subprocess.run(['aws','s3','cp', f\"{S3_OUTPUT_COMBINER}part-0000{i}\", '-'], \n",
    "                       text=True, capture_output=True, check=False)\n",
    "    if p.returncode == 0:\n",
    "        for line in p.stdout.strip().splitlines():\n",
    "            if line:\n",
    "                word, count = line.split('\\t')\n",
    "                combiner_words[word] = combiner_words.get(word, 0) + int(count)\n",
    "\n",
    "print(f\"Total unique words with combiner: {len(combiner_words)}\")\n",
    "\n",
    "sorted_combiner = sorted(combiner_words.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nTop 10 words with combiner:\")\n",
    "for word, count in sorted_combiner[:10]:\n",
    "    print(f\"{word:20s} {count}\")\n",
    "\n",
    "print(f\"\\nResults match without combiner: {all_words == combiner_words}\")\n",
    "print(\"The combiner gives SAME results but with better performance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb934f9e",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "- **S3 AccessDenied**: EMR role needs `s3:ListBucket`, `s3:GetObject`, `s3:PutObject`.\n",
    "- **Output already exists**: delete it (`aws s3 rm ... --recursive`) or change `PREFIX`.\n",
    "- **Jar path missing**: locate with `sudo find /usr/lib -name \"*streaming*.jar\" | head`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
