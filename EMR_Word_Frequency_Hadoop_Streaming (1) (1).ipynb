{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e004fea-3b0b-4589-b0b0-3783020024c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# EMR Hadoop Streaming Word Frequency (MapReduce) — All-in-One Notebook\n",
    "\n",
    "This notebook runs an end-to-end **word frequency** job on **Amazon EMR** using **Hadoop Streaming**.\n",
    "\n",
    "It will:\n",
    "1. Create a small sample `logs.txt`\n",
    "2. Write `mapper.py` and `reducer.py`\n",
    "3. Upload input + code to S3\n",
    "4. Run Hadoop Streaming (`hadoop-streaming.jar`)\n",
    "5. Download and preview results\n",
    "\n",
    "## Assumptions\n",
    "- You are running this notebook **on an EMR cluster** (e.g., EMR Studio / JupyterLab attached to a cluster)\n",
    "- `aws` CLI is available and your EMR role has S3 read/write permissions\n",
    "- `hadoop` CLI is available\n",
    "- Hadoop streaming jar exists at `/usr/lib/hadoop-mapreduce/hadoop-streaming.jar`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff294be",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 0) Configure your S3 paths\n",
    "Set your bucket and prefix. Output path must not already exist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7dfd6f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T20:59:59.037813Z",
     "iopub.status.busy": "2026-01-20T20:59:59.037652Z",
     "iopub.status.idle": "2026-01-20T20:59:59.069184Z",
     "shell.execute_reply": "2026-01-20T20:59:59.068836Z",
     "shell.execute_reply.started": "2026-01-20T20:59:59.037794Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c3f8cf553e44716b6f47867b2d627fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3_INPUT : s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/input/\n",
      "S3_CODE  : s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/code/\n",
      "S3_OUTPUT: s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/output/"
     ]
    }
   ],
   "source": [
    "import os, subprocess, textwrap\n",
    "from pathlib import Path\n",
    "\n",
    "S3_BUCKET = os.environ.get('S3_BUCKET', 'aws-logs-346690756907-us-east-1')\n",
    "PREFIX = os.environ.get('MR_PREFIX', 'mapreduce/wordcount_demo')\n",
    "\n",
    "S3_BASE = f\"s3://{S3_BUCKET}/{PREFIX}\".rstrip('/')\n",
    "S3_INPUT = f\"{S3_BASE}/input/\"\n",
    "S3_CODE = f\"{S3_BASE}/code/\"\n",
    "S3_OUTPUT = f\"{S3_BASE}/output/\"\n",
    "\n",
    "print('S3_INPUT :', S3_INPUT)\n",
    "print('S3_CODE  :', S3_CODE)\n",
    "print('S3_OUTPUT:', S3_OUTPUT)\n",
    "\n",
    "if 'YOUR_BUCKET_NAME' in S3_BUCKET:\n",
    "    print('\\n⚠️  Set S3_BUCKET before running upload/job steps.')\n",
    "\n",
    "def run(cmd, check=True):\n",
    "    print('»', ' '.join(cmd))\n",
    "    p = subprocess.run(cmd, text=True, capture_output=True)\n",
    "    if p.stdout:\n",
    "        print(p.stdout)\n",
    "    if p.stderr:\n",
    "        print(p.stderr)\n",
    "    if check and p.returncode != 0:\n",
    "        raise RuntimeError(f\"Command failed with exit code {p.returncode}\")\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ced403",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1) Sanity checks\n",
    "Verify we can find `aws`, `hadoop`, and the Hadoop Streaming jar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96c57458",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T21:00:02.879149Z",
     "iopub.status.busy": "2026-01-20T21:00:02.878987Z",
     "iopub.status.idle": "2026-01-20T21:00:02.910594Z",
     "shell.execute_reply": "2026-01-20T21:00:02.910203Z",
     "shell.execute_reply.started": "2026-01-20T21:00:02.879131Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1051a34ec12d4738b9b80d63087b8539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? which aws\n",
      "/usr/bin/aws\n",
      "\n",
      "? which hadoop\n",
      "/usr/bin/hadoop\n",
      "\n",
      "Streaming jar exists: True - /usr/lib/hadoop-mapreduce/hadoop-streaming.jar"
     ]
    }
   ],
   "source": [
    "run(['which','aws'], check=False)\n",
    "run(['which','hadoop'], check=False)\n",
    "\n",
    "STREAMING_JAR = Path('/usr/lib/hadoop-mapreduce/hadoop-streaming.jar')\n",
    "print('Streaming jar exists:', STREAMING_JAR.exists(), '-', str(STREAMING_JAR))\n",
    "if not STREAMING_JAR.exists():\n",
    "    print('\\n⚠️ Streaming jar path not found. Try:')\n",
    "    print('   sudo find /usr/lib -name \"*streaming*.jar\" | head')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcf74c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2) Create sample input data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34f7a21b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T21:00:05.673714Z",
     "iopub.status.busy": "2026-01-20T21:00:05.673555Z",
     "iopub.status.idle": "2026-01-20T21:00:05.705528Z",
     "shell.execute_reply": "2026-01-20T21:00:05.705130Z",
     "shell.execute_reply.started": "2026-01-20T21:00:05.673694Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934653dfc8394ad295219b35b26fb920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world hello\n",
      "MapReduce makes scaling easier\n",
      "Hello EMR world\n",
      "Race conditions happen without synchronization"
     ]
    }
   ],
   "source": [
    "logs = textwrap.dedent('''\\\n",
    "Hello world hello\n",
    "MapReduce makes scaling easier\n",
    "Hello EMR world\n",
    "Race conditions happen without synchronization\n",
    "''')\n",
    "with open('logs.txt','w',encoding='utf-8') as f:\n",
    "    f.write(logs)\n",
    "print(logs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e4c60d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3) Write the mapper and reducer\n",
    "- Mapper emits `(word, 1)`\n",
    "- Reducer sums counts per word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b81b617",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T21:00:08.551071Z",
     "iopub.status.busy": "2026-01-20T21:00:08.550911Z",
     "iopub.status.idle": "2026-01-20T21:00:08.581187Z",
     "shell.execute_reply": "2026-01-20T21:00:08.580810Z",
     "shell.execute_reply.started": "2026-01-20T21:00:08.551053Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de23b9163c3410593de9e117e6fed82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? chmod +x mapper.py reducer.py\n",
      "Wrote mapper.py and reducer.py"
     ]
    }
   ],
   "source": [
    "mapper_py = textwrap.dedent('''\\\n",
    "    #!/usr/bin/env python3\n",
    "    import sys\n",
    "    import re\n",
    "\n",
    "    WORD_RE = re.compile(r\"[A-Za-z0-9']+\")\n",
    "\n",
    "    for line in sys.stdin:\n",
    "        for word in WORD_RE.findall(line.lower()):\n",
    "            print(f\"{word}\\t1\")\n",
    "''').lstrip()\n",
    "\n",
    "reducer_py = textwrap.dedent('''\\\n",
    "    #!/usr/bin/env python3\n",
    "    import sys\n",
    "\n",
    "    current_word = None\n",
    "    current_count = 0\n",
    "\n",
    "    for line in sys.stdin:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        word, count = line.split(\"\\t\", 1)\n",
    "        count = int(count)\n",
    "\n",
    "        if current_word == word:\n",
    "            current_count += count\n",
    "        else:\n",
    "            if current_word is not None:\n",
    "                print(f\"{current_word}\\t{current_count}\")\n",
    "            current_word = word\n",
    "            current_count = count\n",
    "\n",
    "    if current_word is not None:\n",
    "        print(f\"{current_word}\\t{current_count}\")\n",
    "''').lstrip()\n",
    "\n",
    "with open('mapper.py','w',encoding='utf-8') as f:\n",
    "    f.write(mapper_py)\n",
    "with open('reducer.py','w',encoding='utf-8') as f:\n",
    "    f.write(reducer_py)\n",
    "\n",
    "run(['chmod','+x','mapper.py','reducer.py'], check=False)\n",
    "print('Wrote mapper.py and reducer.py')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b410ee93",
   "metadata": {},
   "source": [
    "## 4) Quick local test (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce2a1770",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T21:00:15.123662Z",
     "iopub.status.busy": "2026-01-20T21:00:15.123496Z",
     "iopub.status.idle": "2026-01-20T21:00:15.356879Z",
     "shell.execute_reply": "2026-01-20T21:00:15.356469Z",
     "shell.execute_reply.started": "2026-01-20T21:00:15.123641Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "710bb7b3bb3f4b1db60323996e1405ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? cat logs.txt | ./mapper.py | sort | ./reducer.py | sort -k2,2nr | head\n",
      "hello\t3\n",
      "world\t2\n",
      "conditions\t1\n",
      "easier\t1\n",
      "emr\t1\n",
      "happen\t1\n",
      "makes\t1\n",
      "mapreduce\t1\n",
      "race\t1\n",
      "scaling\t1"
     ]
    }
   ],
   "source": [
    "cmd = \"cat logs.txt | ./mapper.py | sort | ./reducer.py | sort -k2,2nr | head\"\n",
    "print('»', cmd)\n",
    "p = subprocess.run(cmd, shell=True, text=True, capture_output=True)\n",
    "print(p.stdout)\n",
    "if p.stderr:\n",
    "    print(p.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc5a862",
   "metadata": {},
   "source": [
    "## 5) Upload input + code to S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f1531e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T21:07:53.943337Z",
     "iopub.status.busy": "2026-01-20T21:07:53.943151Z",
     "iopub.status.idle": "2026-01-20T21:07:55.190946Z",
     "shell.execute_reply": "2026-01-20T21:07:55.190539Z",
     "shell.execute_reply.started": "2026-01-20T21:07:53.943315Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49627d48b9af40d0a97a6ea4f3c09a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? aws s3 cp logs.txt s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/input/\n",
      "Completed 112 Bytes/112 Bytes (2.0 KiB/s) with 1 file(s) remaining\n",
      "upload: ./logs.txt to s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/input/logs.txt\n",
      "\n",
      "? aws s3 cp mapper.py s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/code/\n",
      "Completed 182 Bytes/182 Bytes (1.9 KiB/s) with 1 file(s) remaining\n",
      "upload: ./mapper.py to s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/code/mapper.py\n",
      "\n",
      "? aws s3 cp reducer.py s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/code/\n",
      "Completed 509 Bytes/509 Bytes (8.4 KiB/s) with 1 file(s) remaining\n",
      "upload: ./reducer.py to s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/code/reducer.py\n",
      "\n",
      "Uploaded input and code to S3."
     ]
    }
   ],
   "source": [
    "if 'YOUR_BUCKET_NAME' in S3_BUCKET:\n",
    "    raise ValueError('Set S3_BUCKET to a real bucket name first (or export S3_BUCKET).')\n",
    "\n",
    "run(['aws','s3','cp','logs.txt', S3_INPUT])\n",
    "run(['aws','s3','cp','mapper.py', S3_CODE])\n",
    "run(['aws','s3','cp','reducer.py', S3_CODE])\n",
    "\n",
    "\n",
    "print('Uploaded input and code to S3.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55c785b",
   "metadata": {},
   "source": [
    "## 6) Run the Hadoop Streaming job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06c01bdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T21:08:00.933313Z",
     "iopub.status.busy": "2026-01-20T21:08:00.933139Z",
     "iopub.status.idle": "2026-01-20T21:08:28.269547Z",
     "shell.execute_reply": "2026-01-20T21:08:28.269155Z",
     "shell.execute_reply.started": "2026-01-20T21:08:00.933293Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e730e4d1e24a445ea5d9fefccd0025c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? aws s3 rm s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/output/ --recursive\n",
      "delete: s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/output/part-00001\n",
      "delete: s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/output/part-00000\n",
      "delete: s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/output/part-00002\n",
      "delete: s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/output/_SUCCESS\n",
      "\n",
      "? hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -D mapreduce.job.name=wordcount-streaming -files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py -input s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/input/ -output s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/output/\n",
      "packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-3.4.1-amzn-4.jar] /tmp/streamjob11223802864915766302.jar tmpDir=null\n",
      "\n",
      "2026-01-20 21:08:02,279 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-40-158.ec2.internal/172.31.40.158:8032\n",
      "2026-01-20 21:08:02,331 INFO client.AHSProxy: Connecting to Application History server at ip-172-31-40-158.ec2.internal/172.31.40.158:10200\n",
      "2026-01-20 21:08:02,353 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-40-158.ec2.internal/172.31.40.158:8032\n",
      "2026-01-20 21:08:02,353 INFO client.AHSProxy: Connecting to Application History server at ip-172-31-40-158.ec2.internal/172.31.40.158:10200\n",
      "2026-01-20 21:08:02,402 INFO s3a.EMRFSToS3AConfigMapping: Mapping EMRFS config fs.s3.sts.endpoint to S3A config fs.s3a.assumed.role.sts.endpoint\n",
      "2026-01-20 21:08:02,402 INFO s3a.EMRFSToS3AConfigMapping: Mapping EMRFS config fs.s3.buffer.dir to S3A config fs.s3a.buffer.dir\n",
      "2026-01-20 21:08:02,449 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2026-01-20 21:08:02,490 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 300 second(s).\n",
      "2026-01-20 21:08:02,490 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "2026-01-20 21:08:03,443 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/yarn/.staging/job_1768935873907_0003\n",
      "2026-01-20 21:08:03,599 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library\n",
      "2026-01-20 21:08:03,600 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 049362b7cf53ff5f739d6b1532457f2c6cd495e8]\n",
      "2026-01-20 21:08:03,696 INFO mapred.FileInputFormat: Total input files to process : 2\n",
      "2026-01-20 21:08:03,727 INFO mapreduce.JobSubmitter: number of splits:9\n",
      "2026-01-20 21:08:03,808 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1768935873907_0003\n",
      "2026-01-20 21:08:03,856 INFO mapreduce.JobSubmitter: Executing with tokens: [Kind: YARN_AM_RM_TOKEN, Service: , Ident: (appAttemptId { application_id { id: 1 cluster_timestamp: 1768935873907 } attemptId: 1 } keyId: -560473641)]\n",
      "2026-01-20 21:08:03,906 INFO conf.Configuration: resource-types.xml not found\n",
      "2026-01-20 21:08:03,906 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2026-01-20 21:08:03,939 INFO impl.YarnClientImpl: Submitted application application_1768935873907_0003\n",
      "2026-01-20 21:08:03,956 INFO mapreduce.Job: The url to track the job: http://ip-172-31-40-158.ec2.internal:20888/proxy/application_1768935873907_0003/\n",
      "2026-01-20 21:08:03,957 INFO mapreduce.Job: Running job: job_1768935873907_0003\n",
      "2026-01-20 21:08:07,995 INFO mapreduce.Job: Job job_1768935873907_0003 running in uber mode : false\n",
      "2026-01-20 21:08:07,996 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2026-01-20 21:08:12,027 INFO mapreduce.Job:  map 11% reduce 0%\n",
      "2026-01-20 21:08:15,038 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2026-01-20 21:08:16,044 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2026-01-20 21:08:18,053 INFO mapreduce.Job:  map 78% reduce 0%\n",
      "2026-01-20 21:08:20,059 INFO mapreduce.Job:  map 89% reduce 0%\n",
      "2026-01-20 21:08:21,062 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2026-01-20 21:08:22,065 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "2026-01-20 21:08:25,074 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "2026-01-20 21:08:26,077 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2026-01-20 21:08:26,082 INFO mapreduce.Job: Job job_1768935873907_0003 completed successfully\n",
      "2026-01-20 21:08:26,129 INFO mapreduce.Job: Counters: 60\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=39185\n",
      "\t\tFILE: Number of bytes written=4061885\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1160\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tS3: Number of bytes read=351584\n",
      "\t\tS3: Number of bytes written=28774\n",
      "\t\tS3: Number of read operations=9\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of write operations=6\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=9\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=9\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=217005792\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=102901632\n",
      "\t\tTotal time spent by all map tasks (ms)=37057\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8786\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=37057\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=8786\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=217005792\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=102901632\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3765\n",
      "\t\tMap output records=30684\n",
      "\t\tMap output bytes=215919\n",
      "\t\tMap output materialized bytes=71802\n",
      "\t\tInput split bytes=1160\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3048\n",
      "\t\tReduce shuffle bytes=71802\n",
      "\t\tReduce input records=30684\n",
      "\t\tReduce output records=3048\n",
      "\t\tSpilled Records=61368\n",
      "\t\tShuffled Maps =27\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=27\n",
      "\t\tGC time elapsed (ms)=422\n",
      "\t\tCPU time spent (ms)=12980\n",
      "\t\tPhysical memory (bytes) snapshot=6649335808\n",
      "\t\tVirtual memory (bytes) snapshot=99655606272\n",
      "\t\tTotal committed heap usage (bytes)=5788139520\n",
      "\t\tPeak Map Physical memory (bytes)=613994496\n",
      "\t\tPeak Map Virtual memory (bytes)=7039782912\n",
      "\t\tPeak Reduce Physical memory (bytes)=450310144\n",
      "\t\tPeak Reduce Virtual memory (bytes)=12135251968\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=351584\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=28774\n",
      "2026-01-20 21:08:26,129 INFO streaming.StreamJob: Output directory: s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/output/\n",
      "2026-01-20 21:08:26,133 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "2026-01-20 21:08:26,133 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "2026-01-20 21:08:26,133 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n",
      "\n",
      "CompletedProcess(args=['hadoop', 'jar', '/usr/lib/hadoop-mapreduce/hadoop-streaming.jar', '-D', 'mapreduce.job.name=wordcount-streaming', '-files', 'mapper.py,reducer.py', '-mapper', 'mapper.py', '-reducer', 'reducer.py', '-input', 's3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/input/', '-output', 's3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/output/'], returncode=0, stdout='packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-3.4.1-amzn-4.jar] /tmp/streamjob11223802864915766302.jar tmpDir=null\\n', stderr=\"2026-01-20 21:08:02,279 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-40-158.ec2.internal/172.31.40.158:8032\\n2026-01-20 21:08:02,331 INFO client.AHSProxy: Connecting to Application History server at ip-172-31-40-158.ec2.internal/172.31.40.158:10200\\n2026-01-20 21:08:02,353 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-40-158.ec2.internal/172.31.40.158:8032\\n2026-01-20 21:08:02,353 INFO client.AHSProxy: Connecting to Application History server at ip-172-31-40-158.ec2.internal/172.31.40.158:10200\\n2026-01-20 21:08:02,402 INFO s3a.EMRFSToS3AConfigMapping: Mapping EMRFS config fs.s3.sts.endpoint to S3A config fs.s3a.assumed.role.sts.endpoint\\n2026-01-20 21:08:02,402 INFO s3a.EMRFSToS3AConfigMapping: Mapping EMRFS config fs.s3.buffer.dir to S3A config fs.s3a.buffer.dir\\n2026-01-20 21:08:02,449 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\\n2026-01-20 21:08:02,490 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 300 second(s).\\n2026-01-20 21:08:02,490 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\\n2026-01-20 21:08:03,443 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/yarn/.staging/job_1768935873907_0003\\n2026-01-20 21:08:03,599 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library\\n2026-01-20 21:08:03,600 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 049362b7cf53ff5f739d6b1532457f2c6cd495e8]\\n2026-01-20 21:08:03,696 INFO mapred.FileInputFormat: Total input files to process : 2\\n2026-01-20 21:08:03,727 INFO mapreduce.JobSubmitter: number of splits:9\\n2026-01-20 21:08:03,808 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1768935873907_0003\\n2026-01-20 21:08:03,856 INFO mapreduce.JobSubmitter: Executing with tokens: [Kind: YARN_AM_RM_TOKEN, Service: , Ident: (appAttemptId { application_id { id: 1 cluster_timestamp: 1768935873907 } attemptId: 1 } keyId: -560473641)]\\n2026-01-20 21:08:03,906 INFO conf.Configuration: resource-types.xml not found\\n2026-01-20 21:08:03,906 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\\n2026-01-20 21:08:03,939 INFO impl.YarnClientImpl: Submitted application application_1768935873907_0003\\n2026-01-20 21:08:03,956 INFO mapreduce.Job: The url to track the job: http://ip-172-31-40-158.ec2.internal:20888/proxy/application_1768935873907_0003/\\n2026-01-20 21:08:03,957 INFO mapreduce.Job: Running job: job_1768935873907_0003\\n2026-01-20 21:08:07,995 INFO mapreduce.Job: Job job_1768935873907_0003 running in uber mode : false\\n2026-01-20 21:08:07,996 INFO mapreduce.Job:  map 0% reduce 0%\\n2026-01-20 21:08:12,027 INFO mapreduce.Job:  map 11% reduce 0%\\n2026-01-20 21:08:15,038 INFO mapreduce.Job:  map 33% reduce 0%\\n2026-01-20 21:08:16,044 INFO mapreduce.Job:  map 67% reduce 0%\\n2026-01-20 21:08:18,053 INFO mapreduce.Job:  map 78% reduce 0%\\n2026-01-20 21:08:20,059 INFO mapreduce.Job:  map 89% reduce 0%\\n2026-01-20 21:08:21,062 INFO mapreduce.Job:  map 100% reduce 0%\\n2026-01-20 21:08:22,065 INFO mapreduce.Job:  map 100% reduce 33%\\n2026-01-20 21:08:25,074 INFO mapreduce.Job:  map 100% reduce 67%\\n2026-01-20 21:08:26,077 INFO mapreduce.Job:  map 100% reduce 100%\\n2026-01-20 21:08:26,082 INFO mapreduce.Job: Job job_1768935873907_0003 completed successfully\\n2026-01-20 21:08:26,129 INFO mapreduce.Job: Counters: 60\\n\\tFile System Counters\\n\\t\\tFILE: Number of bytes read=39185\\n\\t\\tFILE: Number of bytes written=4061885\\n\\t\\tFILE: Number of read operations=0\\n\\t\\tFILE: Number of large read operations=0\\n\\t\\tFILE: Number of write operations=0\\n\\t\\tHDFS: Number of bytes read=1160\\n\\t\\tHDFS: Number of bytes written=0\\n\\t\\tHDFS: Number of read operations=9\\n\\t\\tHDFS: Number of large read operations=0\\n\\t\\tHDFS: Number of write operations=0\\n\\t\\tHDFS: Number of bytes read erasure-coded=0\\n\\t\\tS3: Number of bytes read=351584\\n\\t\\tS3: Number of bytes written=28774\\n\\t\\tS3: Number of read operations=9\\n\\t\\tS3: Number of large read operations=0\\n\\t\\tS3: Number of write operations=6\\n\\tJob Counters \\n\\t\\tKilled reduce tasks=1\\n\\t\\tLaunched map tasks=9\\n\\t\\tLaunched reduce tasks=3\\n\\t\\tData-local map tasks=9\\n\\t\\tTotal time spent by all maps in occupied slots (ms)=217005792\\n\\t\\tTotal time spent by all reduces in occupied slots (ms)=102901632\\n\\t\\tTotal time spent by all map tasks (ms)=37057\\n\\t\\tTotal time spent by all reduce tasks (ms)=8786\\n\\t\\tTotal vcore-milliseconds taken by all map tasks=37057\\n\\t\\tTotal vcore-milliseconds taken by all reduce tasks=8786\\n\\t\\tTotal megabyte-milliseconds taken by all map tasks=217005792\\n\\t\\tTotal megabyte-milliseconds taken by all reduce tasks=102901632\\n\\tMap-Reduce Framework\\n\\t\\tMap input records=3765\\n\\t\\tMap output records=30684\\n\\t\\tMap output bytes=215919\\n\\t\\tMap output materialized bytes=71802\\n\\t\\tInput split bytes=1160\\n\\t\\tCombine input records=0\\n\\t\\tCombine output records=0\\n\\t\\tReduce input groups=3048\\n\\t\\tReduce shuffle bytes=71802\\n\\t\\tReduce input records=30684\\n\\t\\tReduce output records=3048\\n\\t\\tSpilled Records=61368\\n\\t\\tShuffled Maps =27\\n\\t\\tFailed Shuffles=0\\n\\t\\tMerged Map outputs=27\\n\\t\\tGC time elapsed (ms)=422\\n\\t\\tCPU time spent (ms)=12980\\n\\t\\tPhysical memory (bytes) snapshot=6649335808\\n\\t\\tVirtual memory (bytes) snapshot=99655606272\\n\\t\\tTotal committed heap usage (bytes)=5788139520\\n\\t\\tPeak Map Physical memory (bytes)=613994496\\n\\t\\tPeak Map Virtual memory (bytes)=7039782912\\n\\t\\tPeak Reduce Physical memory (bytes)=450310144\\n\\t\\tPeak Reduce Virtual memory (bytes)=12135251968\\n\\tShuffle Errors\\n\\t\\tBAD_ID=0\\n\\t\\tCONNECTION=0\\n\\t\\tIO_ERROR=0\\n\\t\\tWRONG_LENGTH=0\\n\\t\\tWRONG_MAP=0\\n\\t\\tWRONG_REDUCE=0\\n\\tFile Input Format Counters \\n\\t\\tBytes Read=351584\\n\\tFile Output Format Counters \\n\\t\\tBytes Written=28774\\n2026-01-20 21:08:26,129 INFO streaming.StreamJob: Output directory: s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/output/\\n2026-01-20 21:08:26,133 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\\n2026-01-20 21:08:26,133 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\\n2026-01-20 21:08:26,133 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\\n\")"
     ]
    }
   ],
   "source": [
    "# Optional cleanup so you can re-run without changing S3_OUTPUT\n",
    "run(['aws','s3','rm', S3_OUTPUT, '--recursive'], check=False)\n",
    "\n",
    "cmd = [\n",
    "    'hadoop','jar', str(STREAMING_JAR),\n",
    "    '-D','mapreduce.job.name=wordcount-streaming',\n",
    "    '-files','mapper.py,reducer.py',\n",
    "    '-mapper','mapper.py',\n",
    "    '-reducer','reducer.py',\n",
    "    '-input', S3_INPUT,\n",
    "    '-output', S3_OUTPUT,\n",
    "]\n",
    "run(cmd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a7afad",
   "metadata": {},
   "source": [
    "## 7) Read results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2415bc17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T21:08:34.023737Z",
     "iopub.status.busy": "2026-01-20T21:08:34.023540Z",
     "iopub.status.idle": "2026-01-20T21:08:35.271330Z",
     "shell.execute_reply": "2026-01-20T21:08:35.270935Z",
     "shell.execute_reply.started": "2026-01-20T21:08:34.023714Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c851c2956724a6dbe50c3a12a5d7229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? aws s3 ls s3://aws-logs-346690756907-us-east-1/mapreduce/wordcount_demo/output/\n",
      "2026-01-20 21:08:25          0 _SUCCESS\n",
      "2026-01-20 21:08:21       9840 part-00000\n",
      "2026-01-20 21:08:24       9780 part-00001\n",
      "2026-01-20 21:08:25       9154 part-00002\n",
      "\n",
      "'as\t1\n",
      "11\t4\n",
      "1991\t1\n",
      "2\t3\n",
      "20\t1\n",
      "5\t4\n",
      "50\t1\n",
      "596\t1\n",
      "6221541\t1\n",
      "8\t5\n",
      "809\t1\n",
      "84116\t1\n",
      "able\t1\n",
      "about\t102\n",
      "accepted\t2\n",
      "account\t1\n",
      "accounts\t1\n",
      "ache\t1\n",
      "actual\t1\n",
      "adjourn\t1\n",
      "advance\t3\n",
      "after\t43\n",
      "again\t83\n",
      "against\t10\n",
      "aged\t1\n",
      "agent\t1\n",
      "ago\t2\n",
      "agony\t1\n",
      "agreed\t1\n",
      "agreement\t18\n",
      "airs\t1\n",
      "alone\t5\n",
      "aloud\t5\n",
      "also\t4\n",
      "alteration\t1\n",
      "altered\t1\n",
      "altogether\t5\n",
      "am\t16\n",
      "ambition\t1\n",
      "among\t12\n",
      "angrily\t9\n",
      "angry\t5\n",
      "ann\t4\n",
      "answered\t4\n",
      "anxiously\t14\n",
      "applause\t1\n",
      "apple\t1\n",
      "archbishop\t2\n",
      "argument\t4\n",
      "arm\t15\n",
      "\n",
      "Top 10:\n",
      "to                   811\n",
      "it                   610\n",
      "she                  553\n",
      "in                   435\n",
      "as                   273\n",
      "t                    218\n",
      "on                   204\n",
      "this                 181\n",
      "out                  118\n",
      "down                 103"
     ]
    }
   ],
   "source": [
    "run(['aws','s3','ls', S3_OUTPUT])\n",
    "p = subprocess.run(['aws','s3','cp', f\"{S3_OUTPUT}part-00000\", '-'], text=True, capture_output=True)\n",
    "out = p.stdout.strip().splitlines()\n",
    "print('\\n'.join(out[:50]))\n",
    "\n",
    "# Top-10 words by count\n",
    "pairs = []\n",
    "for line in out:\n",
    "    w, c = line.split('\\t')\n",
    "    pairs.append((w, int(c)))\n",
    "pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "print('\\nTop 10:')\n",
    "for w, c in pairs[:10]:\n",
    "    print(f\"{w:20s} {c}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb934f9e",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "- **S3 AccessDenied**: EMR role needs `s3:ListBucket`, `s3:GetObject`, `s3:PutObject`.\n",
    "- **Output already exists**: delete it (`aws s3 rm ... --recursive`) or change `PREFIX`.\n",
    "- **Jar path missing**: locate with `sudo find /usr/lib -name \"*streaming*.jar\" | head`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
